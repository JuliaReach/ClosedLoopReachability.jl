using NeuralVerification, MAT
using NeuralVerification: Layer, Network, ReLU, Id

_vec(A::AbstractMatrix) = vec(A)
_vec(A::Number) = [A]
_vec(A::AbstractVector) = A


"""
    readnn(file; key="controller")

Return the neural network using the internal format of NeuralVerification.jl.

### Input

- `file` -- string indicating the location of the file containing the neural network
- `key` -- (optional, default: `"controller"`) key used to search the dictionary

### Output

A Neural Network using the internal format of NeuralVerification.jl

"""
function readnn(file; key="controller")
    vars = matread(file)
    !haskey(vars, key) && throw(ArgumentError("didn't find key $key"))
    dic = vars[key]

    nLayers = Int(dic["number_of_layers"])
    layers = Vector{Layer}(undef, nLayers)
    aF = dic["activation_fcns"]

    for n = 1:nLayers
        W = dic["W"][n]
        b = dic["b"][n]
        if aF[n] == "relu"
            act = ReLU()
        elseif aF[n] == "linear"
            act = Id()
        else
            error("error, aF = $(aF[n]), nLayer = $n")
        end
        layers[n] = Layer(W, _vec(b), act)
    end

    return Network(layers)
end

# ref. Eq (6) in [VER19]
# d(σ(x))/dx = σ(x)*(1-σ(x))
# g(t, x) = σ(tx) = 1 / (1 + exp(-tx))
# dg(t, x)/dt = g'(t, x) = x * g(t, x) * (1 - g(t, x))
@taylorize function sigmoid!(dx, x, p, t)
    xᴶ, xᴾ = x
    dx[1] = zero(xᴶ)
    dx[2] = xᴶ *(xᴾ - xᴾ^2)
end

# d(tanh(x))/dx = 1 - tanh(x)^2
# g(t, x) = tanh(tx)
# dg(t, x)/dt = g'(t, x) = x * (1 - g(t, x)^2)
@taylorize function tanh!(dx, x, p, t)
    xᴶ, xᴾ = x
    dx[1] = zero(xᴶ)
    dx[2] = xᴶ *(1 - xᴾ^2)
end

const HALFINT = IA.Interval(0.5, 0.5)
const ACTFUN = Dict(Tanh => tanh!, Sigmoid => sigmoid!)

# Method: Cartesian decomposition (intervals for each one-dimensional subspace)
function forward(nnet::Network, X0::LazySet, U::LazySet;
                 alg=TMJets(abs_tol=1e-14, orderQ=2, orderT=6))

    # initial states
    xᴾ₀ = _decompose_1D(X0) |> array

    for layer in nnet.layers  # loop over layers
        W = layer.weights
        m, n = size(W)
        b = layer.bias
        act = layer.activation

        xᴶ′ = W * xᴾ₀ + b  # (scalar matrix) * (interval vector) + (scalar vector)
        xᴾ′ = fill(HALFINT, m)

        if act == Id
            xᴾ₀ = copy(xᴶ′)
            continue
        end
        activation! = ACTFUN[act]

        for i = 1:m  # loop over coordinates
            X0i = xᴶ′[i] × xᴾ′[i]
            ivp = @ivp(x' = activation!(x), dim=2, x(0) ∈ X0i)
            sol = RA.solve(ivp, tspan=(0., 1.), alg=alg)
            # interval overapproximation of the final reach-set along
            # dimension 2, which corresponds to xᴾ
            xᴾ_end = sol.F.ext[:xv][end][2]
            xᴾ′[i] = xᴾ_end
        end
        xᴾ₀ = copy(xᴾ′)
    end
    return xᴾ₀
end
